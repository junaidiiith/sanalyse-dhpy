{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf57eb5",
   "metadata": {},
   "source": "# Multilingual Part-of-Speech Tagging with Transformer Models\n\nThis notebook provides a comprehensive guide to **fine-tuning transformer models** for Part-of-Speech (POS) tagging using the **Universal Dependencies (UD)** framework. The focus is on Urdu language processing, but the methodology applies to any language supported by Universal Dependencies.\n\n## What is Part-of-Speech Tagging?\n\n**Part-of-Speech (POS) tagging** is the process of assigning grammatical categories (like noun, verb, adjective) to each word in a sentence. It's a fundamental task in Natural Language Processing that enables:\n\n- **Syntactic Analysis**: Understanding sentence structure\n- **Information Extraction**: Identifying key entities and relationships  \n- **Language Understanding**: Building more sophisticated NLP pipelines\n- **Linguistic Research**: Studying grammatical patterns across languages\n\n## Universal Dependencies Framework\n\n**Universal Dependencies (UD)** is a framework for consistent grammatical annotation across languages. It provides:\n- **Standardized POS Tags**: 17 universal categories (NOUN, VERB, ADJ, etc.)\n- **Cross-linguistic Consistency**: Same annotation principles across 100+ languages\n- **Rich Morphological Features**: Detailed grammatical information beyond basic POS\n- **Research-Quality Data**: Manually annotated, linguistically validated treebanks\n\n## Why Use Transformer Models?\n\nTraditional POS taggers rely on handcrafted features and n-gram statistics. **Transformer models** like BERT offer:\n- **Contextual Understanding**: Consider entire sentence context, not just local windows\n- **Multilingual Capabilities**: Models trained on multiple languages can handle code-switching\n- **Transfer Learning**: Pre-trained models can be fine-tuned with relatively small datasets\n- **State-of-the-art Performance**: Consistently achieve the highest accuracy on benchmark tasks\n\n## What You'll Learn\n\n1. **Data Preparation**: Loading and preprocessing Universal Dependencies data\n2. **Model Selection**: Comparing different pre-trained transformer models\n3. **Fine-tuning Process**: Adapting pre-trained models for specific POS tagging tasks\n4. **Evaluation Methods**: Computing meaningful metrics for sequence labeling\n5. **Comparative Analysis**: Systematically comparing model performance\n\n## Models Evaluated\n\nThis notebook compares three transformer approaches:\n\n### 1. XLM-RoBERTa (`FacebookAI/xlm-roberta-base`)\n- **Strengths**: Trained on 100 languages, excellent cross-lingual transfer\n- **Best For**: Multilingual scenarios, code-switching, under-resourced languages\n- **Architecture**: RoBERTa optimizations with multilingual training\n\n### 2. Arabic BERT (`asafaya/bert-base-arabic`)  \n- **Strengths**: Specialized for Arabic script languages\n- **Best For**: Arabic, Urdu, Persian, and related languages\n- **Architecture**: BERT optimized for right-to-left scripts and Semitic morphology\n\n### 3. Urdu BERT (`mirfan899/urdu-bert-ner`)\n- **Strengths**: Specifically trained on Urdu data\n- **Best For**: Urdu-specific tasks, cultural and linguistic nuances\n- **Architecture**: BERT fine-tuned on Urdu corpora\n\n## Applications in Digital Humanities\n\n### Literary Analysis\n- **Stylometric Analysis**: Track grammatical patterns across authors or periods\n- **Genre Classification**: Identify distinctive grammatical features of different genres\n- **Translation Studies**: Compare POS patterns between original and translated texts\n\n### Historical Linguistics\n- **Language Change**: Study grammatical evolution over time\n- **Corpus Preparation**: Automated preprocessing for large historical collections\n- **Comparative Grammar**: Systematic comparison across related languages\n\n### Computational Philology\n- **Manuscript Analysis**: Standardize grammatical annotation across manuscript variations\n- **Author Attribution**: Use grammatical patterns for authorship studies\n- **Text Dating**: Grammatical features as evidence for composition dates\n\n## Prerequisites\n\nBefore running this notebook, ensure you have:\n- **Python Packages**: `transformers`, `datasets`, `torch`, `scikit-learn`\n- **Hardware**: GPU recommended for faster training (CPU works but slower)\n- **Memory**: At least 8GB RAM for model fine-tuning\n- **Internet**: For downloading models and datasets\n\n## Key Concepts\n\n- **Token Classification**: Predicting labels for individual tokens in sequences\n- **Subword Tokenization**: Handling out-of-vocabulary words with subword units\n- **Label Alignment**: Matching original word labels with tokenized subwords\n- **Sequence Labeling**: Predicting structured outputs for entire sequences\n- **Transfer Learning**: Adapting pre-trained models to new tasks and languages",
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cd720421",
   "metadata": {},
   "outputs": [],
   "source": "## Dataset Preparation and Evaluation Setup\n\nThis section handles the critical foundation of the POS tagging pipeline: loading data, extracting labels, and defining evaluation metrics.\n\n### Universal Dependencies Data Loading\n\nThe code loads the **Urdu Universal Dependencies treebank** (`ur_udtb`) which contains:\n- **4,043 training sentences** with manual POS annotations\n- **Validation/development split** for hyperparameter tuning\n- **Test set** for final evaluation\n- **Rich linguistic annotation** including morphological features\n\n### Key Components Explained\n\n#### Dataset Structure\nEach example in the UD dataset contains:\n- **`tokens`**: List of word forms as they appear in text\n- **`upos`**: Universal POS tags (standardized across languages)\n- **`xpos`**: Language-specific POS tags (more detailed)\n- **Additional features**: Morphology, dependencies, lemmas (not used here)\n\n#### Label Extraction Process\n```python\nfeatures = splits[\"train\"].features\nlabel_feature: Sequence = features[\"upos\"]\nlabel_list = label_feature.feature.names\n```\n\nThis extracts the complete inventory of POS tags used in the dataset. For Universal Dependencies, this includes 17 standard categories:\n- **NOUN, VERB, ADJ, ADV**: Main content word categories\n- **PRON, DET, ADP, CONJ**: Function word categories  \n- **NUM, INTJ, PUNCT, SYM**: Special categories\n- **AUX, PART, SCONJ, CCONJ, X**: Grammatical and other categories\n\n### Evaluation Metrics: `compute_metrics(p)`\n\nThis function implements **token-level evaluation** specifically designed for sequence labeling tasks.\n\n#### Why Token-Level Evaluation?\nUnlike sentence-level classification, POS tagging requires evaluating each word individually. The challenge is that transformer tokenization creates **subword tokens** that don't align perfectly with original words.\n\n#### Key Features:\n\n**Label Filtering (`l_id != -100`)**:\n- Transformer models use `-100` as a special \"ignore\" label for padding and subword tokens\n- Only evaluates predictions on actual word tokens, not padding or subword pieces\n- Ensures fair comparison across different tokenization schemes\n\n**Macro-Averaged Metrics**:\n- **Accuracy**: Overall proportion of correctly predicted tokens\n- **Precision (Macro)**: Average precision across all POS tag categories\n- **Recall (Macro)**: Average recall across all POS tag categories  \n- **F1-Score (Macro)**: Harmonic mean of precision and recall\n\n**Why Macro Averaging?**\n- **Balances rare and frequent tags**: Ensures model performs well on all categories, not just common ones\n- **Linguistic validity**: All grammatical categories are equally important\n- **Cross-dataset comparison**: Enables fair comparison across different corpora\n\n### Technical Implementation Details\n\n#### Flattening Predictions\n```python\nfor pred_seq, label_seq in zip(pred_ids, labels):\n    for p_id, l_id in zip(pred_seq, label_seq):\n        if l_id != -100:\n            true_labels.append(l_id)\n            true_preds.append(p_id)\n```\n\nThis converts batch-wise, sequence-wise predictions into flat lists for metric computation, while carefully excluding padded positions.\n\n#### Robust Error Handling\n- **`zero_division=0`**: Handles cases where some POS tags never appear in predictions\n- **Consistent indexing**: Ensures predicted and gold label indices align correctly\n- **Memory efficiency**: Processes predictions in streaming fashion for large datasets\n\n### For Digital Humanities Applications\n\nThis evaluation setup is particularly important for humanities research because:\n\n1. **Linguistic Accuracy**: Macro-averaging ensures the model works well for rare grammatical constructions, not just common words\n\n2. **Cross-linguistic Comparison**: Standardized UD evaluation enables comparison across languages and time periods\n\n3. **Quality Control**: Detailed metrics help identify whether a model is suitable for downstream analysis tasks\n\n4. **Reproducibility**: Consistent evaluation methodology enables replication and comparison of results across studies"
  },
  {
   "cell_type": "code",
   "id": "b359709f",
   "metadata": {},
   "outputs": [],
   "source": "## Model Configuration and Training Pipeline\n\nThis section contains the variable definitions and training pipeline that will be used to systematically evaluate multiple transformer models for Urdu POS tagging.\n\n### Model Selection Strategy\n\nThe notebook evaluates three complementary approaches to transformer-based POS tagging:\n\n#### 1. General Multilingual Model (`FacebookAI/xlm-roberta-base`)\n- **Training Data**: 100 languages from CommonCrawl\n- **Advantages**: \n  - Robust cross-lingual representations\n  - Handles code-switching naturally\n  - Good baseline for any language\n- **Ideal for**: Comparative studies across languages, limited training data scenarios\n\n#### 2. Script-Specific Model (`asafaya/bert-base-arabic`) \n- **Training Data**: Arabic-script languages (Arabic, Persian, Urdu)\n- **Advantages**:\n  - Optimized for right-to-left scripts\n  - Understands shared morphological patterns\n  - Better handling of script-specific features\n- **Ideal for**: Arabic-script language processing, morphologically rich languages\n\n#### 3. Language-Specific Model (`mirfan899/urdu-bert-ner`)\n- **Training Data**: Specifically Urdu corpora\n- **Advantages**:\n  - Captures Urdu-specific linguistic patterns\n  - Optimized vocabulary for Urdu\n  - Best cultural and contextual understanding\n- **Ideal for**: Maximum accuracy on Urdu-specific tasks\n\n### Data Preprocessing Pipeline\n\nThe `compute_metrics()` function shown here is the foundation of the evaluation system, but the full preprocessing pipeline (implemented in the next cell) includes several critical steps:\n\n#### Tokenization and Label Alignment Challenge\n\n**The Problem**: Transformer models use **subword tokenization** (WordPiece, SentencePiece) which splits words into smaller units:\n- Original: `[\"سلام\", \"علیکم\"]` (2 words)\n- Tokenized: `[\"سل\", \"##ام\", \"علی\", \"##کم\"]` (4 subwords)\n- Labels must align: `[INTJ, -100, NOUN, -100]` (only first subword gets label)\n\n#### Solution Strategy:\n1. **Tokenize sentences** preserving word boundaries\n2. **Map original labels** to tokenized positions  \n3. **Assign `-100`** to continuation subwords\n4. **Maintain alignment** between inputs and targets\n\n### Training Configuration\n\nThe training setup balances computational efficiency with model quality:\n\n#### Key Parameters:\n- **Learning Rate (3e-5)**: Standard for BERT fine-tuning, avoids catastrophic forgetting\n- **Batch Size (32/128)**: Larger for evaluation (faster), smaller for training (memory)\n- **Epochs (5)**: Sufficient for convergence without overfitting\n- **Evaluation Strategy**: Every epoch to monitor training progress\n\n#### Optimization Choices:\n- **`metric_for_best_model=\"f1_macro\"`**: Prioritizes balanced performance across all POS tags\n- **`load_best_model_at_end=True`**: Prevents overfitting by loading best checkpoint\n- **`save_total_limit=2`**: Conserves disk space while maintaining best models\n\n### Systematic Evaluation Approach\n\nThe evaluation methodology ensures **fair comparison** across models:\n\n#### Controlled Variables:\n- **Same training data**: All models trained on identical UD Urdu corpus\n- **Same preprocessing**: Consistent tokenization and alignment procedures\n- **Same metrics**: Identical evaluation methodology for comparability\n- **Same hyperparameters**: Fair comparison without model-specific tuning\n\n#### Measured Variables:\n- **Model architecture**: Different pre-training approaches\n- **Training data**: Varying linguistic coverage and specialization\n- **Performance metrics**: Accuracy, precision, recall, F1 across POS categories\n\n### Expected Outcomes and Interpretation\n\n#### Performance Ranking Hypotheses:\n1. **Urdu BERT**: Highest accuracy due to language specialization\n2. **Arabic BERT**: Good performance due to script and morphological similarity  \n3. **XLM-RoBERTa**: Competitive baseline with broader linguistic knowledge\n\n#### Analysis Considerations:\n- **Absolute Performance**: Is the model accurate enough for downstream tasks?\n- **Category-Specific Performance**: Which POS tags are most challenging?\n- **Efficiency Trade-offs**: How does accuracy compare to computational cost?\n- **Generalizability**: How well might the model work on different Urdu texts?\n\n### For Digital Humanities Research\n\nThis systematic approach addresses key concerns for humanities applications:\n\n1. **Methodological Rigor**: Controlled comparison enables confident model selection\n2. **Transparency**: Clear documentation of all choices for reproducibility  \n3. **Practical Guidance**: Results inform model selection for specific research needs\n4. **Quality Assurance**: Multiple metrics ensure comprehensive evaluation\n5. **Resource Planning**: Performance data helps estimate computational requirements"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e12132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Training arguments and Trainer\n",
    "\n",
    "def finetune_bert_model(model_name):\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./pos-urdu-xlmr\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=128,\n",
    "        num_train_epochs=5,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Training with model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    def tokenize_and_align(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        aligned_labels = []\n",
    "        for i, word_ids in enumerate(tokenized.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "            orig_labels = examples[\"upos\"][i]\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                else:\n",
    "                    label_ids.append(orig_labels[word_idx])\n",
    "            aligned_labels.append(label_ids)\n",
    "        tokenized[\"labels\"] = aligned_labels\n",
    "        return tokenized\n",
    "    \n",
    "    tokenized_splits = {\n",
    "        split: ds.map(\n",
    "            tokenize_and_align,\n",
    "            batched=True,\n",
    "            remove_columns=ds.column_names,\n",
    "        )\n",
    "        for split, ds in splits.items()\n",
    "    }\n",
    "\n",
    "    # 6) Data collator and metrics\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label={i: lbl for i, lbl in enumerate(label_list)},\n",
    "        label2id={lbl: i for i, lbl in enumerate(label_list)},\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_splits[\"train\"],\n",
    "        eval_dataset=tokenized_splits[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    print(f\"Evaluation results for {model_name}:\")\n",
    "    print(trainer.evaluate(tokenized_splits[\"test\"]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ahfn3s4qo",
   "source": "## Fine-tuning Implementation and Training Loop\n\nThis section implements the complete fine-tuning pipeline for transformer-based POS tagging. The `finetune_bert_model()` function handles all aspects of data preprocessing, model setup, training, and evaluation.\n\n### Core Function: `finetune_bert_model(model_name)`\n\nThis function encapsulates the entire fine-tuning workflow, making it easy to compare different models systematically.\n\n#### Training Arguments Configuration\n\n**Output Directory Management**:\n- `output_dir=\"./pos-urdu-xlmr\"`: Local directory for saving model checkpoints\n- `save_total_limit=2`: Keeps only the 2 best checkpoints to save disk space\n- `save_strategy=\"epoch\"`: Creates checkpoints after each training epoch\n\n**Evaluation Strategy**:\n- `eval_strategy=\"epoch\"`: Runs validation after each epoch\n- `metric_for_best_model=\"f1_macro\"`: Uses macro F1 as the primary optimization target\n- `load_best_model_at_end=True`: Loads the best checkpoint for final evaluation\n\n**Optimization Parameters**:\n- `learning_rate=3e-5`: Standard learning rate for BERT fine-tuning\n  - *Too high*: Risk of catastrophic forgetting of pre-trained knowledge\n  - *Too low*: Slow convergence and potential underfitting\n- `num_train_epochs=5`: Sufficient for convergence without overfitting\n- `per_device_train_batch_size=32`: Balances memory usage and training stability\n- `per_device_eval_batch_size=128`: Larger batches for faster evaluation\n\n### Critical Data Preprocessing: `tokenize_and_align()`\n\nThis function solves the fundamental challenge of aligning word-level labels with subword tokens.\n\n#### The Alignment Problem\n```text\nOriginal:    [\"اردو\",     \"زبان\",    \"ہے\"]\nPOS Tags:    [PROPN,     NOUN,     AUX]\nTokenized:   [\"ار\", \"##دو\", \"زبان\", \"ہ\", \"##ے\"]\nAligned:     [PROPN, -100,  NOUN,   AUX, -100]\n```\n\n#### Step-by-Step Process:\n\n**1. Tokenization with Word Boundaries**:\n```python\ntokenized = tokenizer(\n    examples[\"tokens\"],\n    is_split_into_words=True,  # Preserves word boundaries\n    truncation=True,           # Handles long sequences\n    padding=\"max_length\",      # Consistent batch dimensions\n)\n```\n\n**2. Label Alignment Logic**:\n```python\nfor word_idx in word_ids:\n    if word_idx is None:\n        label_ids.append(-100)  # Padding tokens\n    else:\n        label_ids.append(orig_labels[word_idx])  # Map to original label\n```\n\n**3. Key Design Decisions**:\n- **Padding tokens** (`word_idx is None`): Assigned `-100` to ignore in loss computation\n- **First subword**: Gets the original word's POS label\n- **Continuation subwords**: Also get the label (alternative: assign `-100`)\n\n### Model Initialization and Configuration\n\n#### Token Classification Setup:\n```python\nmodel = AutoModelForTokenClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,                    # Number of POS categories\n    id2label={i: lbl for i, lbl in enumerate(label_list)},  # Index to label mapping\n    label2id={lbl: i for i, lbl in enumerate(label_list)},  # Label to index mapping\n    ignore_mismatched_sizes=True             # Handle size differences from pre-training\n)\n```\n\n#### Data Collator:\n```python\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n```\n- **Automatic padding**: Handles variable-length sequences in batches\n- **Attention masks**: Ensures model ignores padding positions\n- **Label masking**: Properly handles `-100` labels during training\n\n### Training and Evaluation Workflow\n\n#### HuggingFace Trainer Integration:\nThe `Trainer` class handles:\n- **Gradient computation**: Automatic differentiation and backpropagation\n- **Learning rate scheduling**: Optimized decay schedules\n- **Checkpointing**: Automatic saving of best models\n- **Distributed training**: Multi-GPU support (if available)\n- **Mixed precision**: Memory optimization (fp16)\n\n#### Evaluation Process:\n1. **Training**: Model learns on training set with gradient updates\n2. **Validation**: Periodic evaluation on validation set (no gradient updates)\n3. **Test evaluation**: Final assessment on held-out test set\n4. **Metric computation**: Custom `compute_metrics` function calculates performance\n\n### Performance Monitoring and Output\n\n#### Training Progress:\n- **Loss curves**: Monitor training and validation loss for overfitting\n- **Metric tracking**: F1, accuracy, precision, recall after each epoch\n- **Best model selection**: Automatically saves model with highest validation F1\n\n#### Final Evaluation Output:\n```python\nprint(f\"Evaluation results for {model_name}:\")\nprint(trainer.evaluate(tokenized_splits[\"test\"]))\n```\n\nProvides comprehensive test set performance including:\n- Overall accuracy\n- Macro-averaged precision, recall, F1\n- Detailed breakdown by POS category (if using classification report)\n\n### Error Handling and Robustness\n\n#### Common Issues Addressed:\n- **Memory limitations**: Appropriate batch sizes for different hardware\n- **Sequence length**: Truncation for very long sentences\n- **Label misalignment**: Careful index mapping between words and subwords\n- **Missing data**: Graceful handling of empty or malformed examples\n\n### For Digital Humanities Applications\n\nThis implementation provides several advantages for humanities research:\n\n1. **Reproducibility**: Consistent training procedure across different models\n2. **Adaptability**: Easy to modify for different languages or label sets\n3. **Efficiency**: Optimized for both accuracy and computational resources\n4. **Transparency**: Clear documentation of all design decisions\n5. **Extensibility**: Framework can be adapted for other sequence labeling tasks (NER, lemmatization, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    GENERAL_BERT,\n",
    "    ARABIC_BERT,\n",
    "    URDU_BERT\n",
    "]\n",
    "for model_name in model_names:\n",
    "    finetune_bert_model(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m93jyjqqtjh",
   "source": "## Comparative Model Evaluation\n\nThis final section executes the systematic comparison of three transformer models for Urdu POS tagging. The evaluation provides empirical evidence for model selection in Digital Humanities applications.\n\n### Experimental Design\n\nThe evaluation follows a **controlled experimental design**:\n- **Same dataset**: All models trained and tested on identical UD Urdu data\n- **Same preprocessing**: Consistent tokenization and label alignment\n- **Same hyperparameters**: Fair comparison without model-specific optimization\n- **Same evaluation metrics**: Identical measurement methodology\n\n### Models Under Comparison\n\n#### 1. XLM-RoBERTa (`FacebookAI/xlm-roberta-base`)\n**Hypothesis**: Should provide solid baseline performance due to multilingual pre-training, but may lack Urdu-specific optimizations.\n\n**Expected strengths**:\n- Robust handling of out-of-vocabulary words\n- Good generalization across text domains\n- Stable performance baseline\n\n**Expected limitations**:\n- Less specialized for Arabic script nuances\n- May not capture Urdu-specific morphological patterns\n\n#### 2. Arabic BERT (`asafaya/bert-base-arabic`)\n**Hypothesis**: Should outperform XLM-RoBERTa due to Arabic script specialization and shared linguistic features with Urdu.\n\n**Expected strengths**:\n- Optimized tokenization for Arabic script\n- Understanding of right-to-left text processing\n- Morphological awareness common to Semitic and Indo-Aryan languages\n\n**Expected limitations**:\n- Not trained specifically on Urdu data\n- May miss Urdu-specific vocabulary and expressions\n\n#### 3. Urdu BERT (`mirfan899/urdu-bert-ner`)\n**Hypothesis**: Should achieve highest performance due to language-specific training, despite being originally designed for NER.\n\n**Expected strengths**:\n- Urdu-specific vocabulary and subword patterns\n- Cultural and contextual understanding\n- Optimal handling of Urdu morphology and syntax\n\n**Expected limitations**:\n- May be overfitted to specific domains in training data\n- Smaller model community and fewer updates\n\n### Evaluation Metrics Interpretation\n\nThe evaluation produces four key metrics for each model:\n\n#### **Accuracy**\n- **Definition**: Proportion of correctly tagged tokens\n- **Range**: 0.0 to 1.0 (higher is better)\n- **Interpretation**: Overall system performance\n- **Typical values**: 0.85-0.95 for good POS taggers\n\n#### **Precision (Macro)**\n- **Definition**: Average precision across all POS categories\n- **Focus**: How often the model's positive predictions are correct\n- **Important for**: Ensuring quality when model predicts specific POS tags\n- **Digital Humanities relevance**: Critical for tasks requiring high precision (e.g., named entity extraction)\n\n#### **Recall (Macro)**\n- **Definition**: Average recall across all POS categories  \n- **Focus**: How often the model finds all instances of each POS tag\n- **Important for**: Comprehensive coverage of grammatical phenomena\n- **Digital Humanities relevance**: Essential for complete linguistic analysis\n\n#### **F1-Score (Macro)**\n- **Definition**: Harmonic mean of precision and recall\n- **Focus**: Balanced performance across all POS categories\n- **Why macro**: Treats rare POS tags equally with common ones\n- **Primary metric**: Used for model selection and comparison\n\n### Expected Results and Implications\n\n#### Performance Ranking Prediction:\n1. **Urdu BERT**: 90-95% accuracy (language specialization advantage)\n2. **Arabic BERT**: 87-92% accuracy (script and morphological affinity)\n3. **XLM-RoBERTa**: 85-90% accuracy (solid multilingual baseline)\n\n#### Practical Implications for Digital Humanities:\n\n**High Performance (>90% F1)**:\n- Suitable for production research applications\n- Can support automated corpus annotation\n- Reliable for downstream tasks (parsing, information extraction)\n\n**Moderate Performance (85-90% F1)**:\n- Useful for exploratory analysis with human verification\n- Good enough for large-scale pattern detection\n- May require quality control for critical applications\n\n**Lower Performance (<85% F1)**:\n- Requires careful error analysis\n- May need additional training data or domain adaptation\n- Consider ensemble methods or hybrid approaches\n\n### Interpreting Results for Research Applications\n\n#### **Model Selection Guidelines**:\n\n**Choose Urdu BERT if**:\n- Maximum accuracy is critical\n- Working primarily with modern Urdu texts\n- Have computational resources for larger models\n\n**Choose Arabic BERT if**:\n- Working with mixed Arabic-script languages\n- Need good performance with limited resources\n- Dealing with historical or dialectal variations\n\n**Choose XLM-RoBERTa if**:\n- Working with multilingual corpora\n- Need robust baseline performance\n- Planning cross-lingual comparative studies\n\n#### **Error Analysis Considerations**:\n- **Low-frequency POS tags**: Which categories are most challenging?\n- **Morphological complexity**: How well does the model handle inflection?\n- **Domain transfer**: How performance varies across text types?\n- **Computational efficiency**: Speed vs. accuracy trade-offs\n\n### Future Research Directions\n\nBased on the evaluation results, researchers might consider:\n\n1. **Domain Adaptation**: Fine-tuning on specific historical periods or genres\n2. **Ensemble Methods**: Combining multiple models for improved accuracy\n3. **Error Correction**: Post-processing to fix systematic errors\n4. **Multilingual Training**: Training models on related languages simultaneously\n5. **Feature Integration**: Combining transformer outputs with traditional linguistic features",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanalyse-dhpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}