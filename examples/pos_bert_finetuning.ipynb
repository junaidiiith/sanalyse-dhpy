{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf57eb5",
   "metadata": {},
   "source": [
    "# Multilingual POS Tagging with Transformers\n",
    "\n",
    "This notebook demonstrates how to perform Part-of-Speech (POS) tagging on the Universal Dependencies Urdu dataset using various transformer-based models. The workflow includes data loading, preprocessing, model training, and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Imports**  \n",
    "    Essential libraries for data handling, model training, and evaluation are imported, including HuggingFace Transformers, Datasets, and scikit-learn metrics.\n",
    "\n",
    "2. **Dataset Preparation**  \n",
    "    - The Universal Dependencies Urdu dataset is loaded.\n",
    "    - Data splits for training, validation, and testing are created.\n",
    "    - POS tag labels are extracted for use in model training.\n",
    "\n",
    "3. **Metric Computation**  \n",
    "    A custom function computes accuracy, precision, recall, and F1-score at the token level, ignoring padding tokens.\n",
    "\n",
    "4. **Model Training and Evaluation**  \n",
    "    - Three transformer models are evaluated: XLM-RoBERTa, Arabic BERT, and Urdu BERT.\n",
    "    - For each model:\n",
    "      - The tokenizer is loaded and used to tokenize and align labels with tokens.\n",
    "      - Data is preprocessed and mapped for model input.\n",
    "      - The model is trained using HuggingFace's Trainer API.\n",
    "      - Evaluation metrics are computed and displayed for each model.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Points\n",
    "\n",
    "- **Tokenization and Label Alignment:**  \n",
    "  Special care is taken to align original word-level labels with tokenized subwords, assigning `-100` to tokens that should be ignored during loss computation.\n",
    "\n",
    "- **Evaluation:**  \n",
    "  Macro-averaged metrics are used to provide a balanced view of model performance across all POS tags.\n",
    "\n",
    "- **Reproducibility:**  \n",
    "  Training arguments and model checkpoints are managed for consistent and repeatable experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [Universal Dependencies Project](https://universaldependencies.org/)\n",
    "- [scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provides a template for multilingual sequence labeling tasks using state-of-the-art transformer models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd720421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation for Data Preparation and Metric Computation\n",
    "\"\"\"\n",
    "This section prepares the Universal Dependencies Urdu dataset for POS tagging.\n",
    "- Loads the dataset and creates train/validation/test splits.\n",
    "- Extracts the list of POS tag labels and their count.\n",
    "- Defines a function `compute_metrics` to evaluate model predictions using accuracy, precision, recall, and F1-score at the token level, ignoring padding tokens (-100).\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset, Sequence\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b359709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation for Model Training and Evaluation\n",
    "\"\"\"\n",
    "This section iterates over three transformer models (XLM-RoBERTa, Arabic BERT, Urdu BERT) for POS tagging:\n",
    "- Loads each model and its tokenizer.\n",
    "- Defines a function to tokenize input sentences and align POS labels with subword tokens.\n",
    "- Preprocesses the dataset splits for model input.\n",
    "- Sets up the Trainer with appropriate arguments, data collator, and metric computation.\n",
    "- Trains each model and evaluates its performance on the test set, printing out the results.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "GENERAL_BERT = \"FacebookAI/xlm-roberta-base\"\n",
    "ARABIC_BERT = \"asafaya/bert-base-arabic\"\n",
    "URDU_BERT = 'mirfan899/urdu-bert-ner'\n",
    "UR_LANGUAGE = 'ur_udtb'\n",
    "\n",
    "# 1) Load UD Urdu and get splits\n",
    "ud = load_dataset(\"universal_dependencies\", UR_LANGUAGE)\n",
    "\n",
    "splits = {\n",
    "    \"train\": ud[\"train\"],\n",
    "    \"validation\": ud.get(\"validation\", ud.get(\"dev\")),\n",
    "    \"test\": ud[\"test\"],\n",
    "}\n",
    "\n",
    "# 2) Extract labels\n",
    "features = splits[\"train\"].features\n",
    "label_feature: Sequence = features[\"upos\"]\n",
    "label_list = label_feature.feature.names  # list of string labels\n",
    "num_labels = len(label_list)\n",
    "\n",
    "\n",
    "# 7) Compute metrics without seqeval (token-level classification)\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    pred_ids = preds.argmax(-1)\n",
    "    # Flatten\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_seq, label_seq in zip(pred_ids, labels):\n",
    "        for p_id, l_id in zip(pred_seq, label_seq):\n",
    "            if l_id != -100:\n",
    "                true_labels.append(l_id)\n",
    "                true_preds.append(p_id)\n",
    "    accuracy = accuracy_score(true_labels, true_preds)\n",
    "    precision = precision_score(true_labels, true_preds, average='macro', zero_division=0)\n",
    "    recall = recall_score(true_labels, true_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(true_labels, true_preds, average='macro', zero_division=0)\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e12132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Training arguments and Trainer\n",
    "\n",
    "def finetune_bert_model(model_name):\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./pos-urdu-xlmr\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=128,\n",
    "        num_train_epochs=5,\n",
    "        logging_dir=\"./logs\",\n",
    "        save_total_limit=2,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    print(f\"Training with model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    def tokenize_and_align(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"tokens\"],\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        aligned_labels = []\n",
    "        for i, word_ids in enumerate(tokenized.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "            orig_labels = examples[\"upos\"][i]\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                else:\n",
    "                    label_ids.append(orig_labels[word_idx])\n",
    "            aligned_labels.append(label_ids)\n",
    "        tokenized[\"labels\"] = aligned_labels\n",
    "        return tokenized\n",
    "    \n",
    "    tokenized_splits = {\n",
    "        split: ds.map(\n",
    "            tokenize_and_align,\n",
    "            batched=True,\n",
    "            remove_columns=ds.column_names,\n",
    "        )\n",
    "        for split, ds in splits.items()\n",
    "    }\n",
    "\n",
    "    # 6) Data collator and metrics\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label={i: lbl for i, lbl in enumerate(label_list)},\n",
    "        label2id={lbl: i for i, lbl in enumerate(label_list)},\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_splits[\"train\"],\n",
    "        eval_dataset=tokenized_splits[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    print(f\"Evaluation results for {model_name}:\")\n",
    "    print(trainer.evaluate(tokenized_splits[\"test\"]))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    GENERAL_BERT,\n",
    "    ARABIC_BERT,\n",
    "    URDU_BERT\n",
    "]\n",
    "for model_name in model_names:\n",
    "    finetune_bert_model(model_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanalyse-dhpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
