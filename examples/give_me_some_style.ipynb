{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4mullv99pxa",
   "source": "# Text Style Analysis: Readability Features and Embeddings\n\nThis notebook demonstrates comprehensive **text style analysis** techniques, focusing on extracting quantitative features that capture different aspects of writing style. The analysis combines traditional readability metrics with modern embedding approaches, making it particularly valuable for Digital Humanities research into literary style, authorship, and textual complexity.\n\n## What is Text Style Analysis?\n\n**Text style analysis** examines the quantifiable characteristics of writing that reflect an author's unique voice, the complexity of the text, and various linguistic patterns. Unlike content analysis (which focuses on *what* is said), style analysis focuses on *how* it is said.\n\nKey dimensions of style include:\n- **Readability**: How easy or difficult a text is to understand\n- **Lexical Richness**: Vocabulary diversity and sophistication  \n- **Syntactic Complexity**: Sentence structure and grammatical patterns\n- **Semantic Density**: Information density and conceptual complexity\n\n## What You'll Learn\n\n1. **Traditional Readability Metrics**: Computing established formulas like Flesch-Kincaid, SMOG, etc.\n2. **Advanced Lexical Features**: Type-token ratios, hapax legomena, entropy measures\n3. **Modern Embeddings**: Word2Vec, BERT, and OpenAI embeddings for style representation\n4. **Feature Integration**: Combining multiple approaches for comprehensive style profiles\n\n## Applications in Digital Humanities\n\n### Literary Analysis\n- **Authorship Attribution**: Distinguish between different authors based on style\n- **Period Analysis**: Track stylistic changes across historical periods\n- **Genre Classification**: Identify characteristics of different literary genres\n- **Translation Studies**: Compare stylistic features across languages\n\n### Educational Research  \n- **Text Difficulty Assessment**: Evaluate reading level for pedagogical materials\n- **Writing Development**: Track changes in student writing complexity over time\n- **Curriculum Design**: Match texts to appropriate reading levels\n\n### Historical Studies\n- **Document Dating**: Use stylistic features to estimate text composition dates\n- **Authenticity Verification**: Detect potentially forged or misattributed documents\n- **Social Analysis**: Examine how writing style reflects social class or education\n\n## Prerequisites\n\nBefore running this notebook, ensure you have:\n- Required Python packages: `nltk`, `textstat`, `transformers`, `openai`, `gensim`\n- Pre-trained embeddings (optional): Google News Word2Vec model\n- OpenAI API key (for modern embeddings)\n- NLTK data packages (punkt, averaged_perceptron_tagger)\n\n## Key Concepts\n\n- **Readability Formulas**: Mathematical models predicting text difficulty\n- **Type-Token Ratio (TTR)**: Vocabulary diversity measure\n- **Hapax Legomena**: Words appearing only once in a text\n- **Entropy**: Information-theoretic measure of vocabulary distribution\n- **Embeddings**: Dense vector representations capturing semantic/stylistic properties",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "6cab5500",
   "metadata": {},
   "source": [
    "# TCR Feature Extraction: Readability Formulas\n",
    "\n",
    "This module provides functions to compute standard readability metrics for a given text using the `textstat` library. These metrics estimate how easy or difficult a text is to read, often expressed as a U.S. grade level or a score.\n",
    "\n",
    "**Features computed:**\n",
    "- SMOG Index\n",
    "- Automated Readability Index (ARI)\n",
    "- Dale-Chall Readability Score\n",
    "- Linsear Write Formula\n",
    "- Gunning-Fog Index\n",
    "- Coleman-Liau Index\n",
    "- Flesch Reading Ease\n",
    "- Flesch Kincaid Grade Level\n",
    "\n",
    "**Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9190db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SMOG': 3.1291, 'ARI': 3.151578947368421, 'Dale-Chall': 9.094015789473683, 'Linsear Write': 3.75, 'Gunning-Fog': 3.8000000000000003, 'Coleman-Liau': 4.894736842105264, 'Flesch Reading Ease': 90.32934210526317, 'Flesch-Kincaid Grade': 3.0202631578947354, 'AvgChars/Word': 3.8095238095238093, 'StdChars/Word': 1.8157786633273922, 'AvgWords/Sentence': 10.5, 'StdWords/Sentence': 0.5, 'HapaxLegomena': 17, 'DisLegomena': 2, 'Entropy': 4.20184123230257, 'Perplexity': 18.402644982465112, 'TTR': 0.9047619047619048, 'MATTR': 0, 'MTLD': 7.212616822429907, 'FunctionalDiversity': 1.625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sali/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import textstat\n",
    "import math\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure necessary NLTK data is available\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_readability_scores(text):\n",
    "    \"\"\"\n",
    "    Compute core readability metrics.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'SMOG': textstat.smog_index(text),\n",
    "        'ARI': textstat.automated_readability_index(text),\n",
    "        'Dale-Chall': textstat.dale_chall_readability_score(text),\n",
    "        'Linsear Write': textstat.linsear_write_formula(text),\n",
    "        'Gunning-Fog': textstat.gunning_fog(text),\n",
    "        'Coleman-Liau': textstat.coleman_liau_index(text),\n",
    "        'Flesch Reading Ease': textstat.flesch_reading_ease(text),\n",
    "        'Flesch-Kincaid Grade': textstat.flesch_kincaid_grade(text)\n",
    "    }\n",
    "\n",
    "def get_length_stats(text):\n",
    "    \"\"\"\n",
    "    Average word length, sentence length, and their standard deviations.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chars_per_word = [len(w) for w in words]\n",
    "    words_per_sent = [len(nltk.word_tokenize(s)) for s in sentences]\n",
    "    \n",
    "    return {\n",
    "        'AvgChars/Word': statistics.mean(chars_per_word),\n",
    "        'StdChars/Word': statistics.pstdev(chars_per_word),\n",
    "        'AvgWords/Sentence': statistics.mean(words_per_sent),\n",
    "        'StdWords/Sentence': statistics.pstdev(words_per_sent)\n",
    "    }\n",
    "\n",
    "def get_hapax_dislegomena(text):\n",
    "    \"\"\"\n",
    "    Count words occurring exactly once (hapax) or twice (dis legomena).\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    freqs = Counter(tokens)\n",
    "    return {\n",
    "        'HapaxLegomena': sum(1 for c in freqs.values() if c == 1),\n",
    "        'DisLegomena': sum(1 for c in freqs.values() if c == 2)\n",
    "    }\n",
    "\n",
    "def get_entropy_perplexity(text):\n",
    "    \"\"\"\n",
    "    Shannon entropy and derived perplexity of the token distribution.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    freqs = Counter(tokens)\n",
    "    total = len(tokens)\n",
    "    entropy = -sum((c/total) * math.log2(c/total) for c in freqs.values())\n",
    "    perplexity = 2**entropy\n",
    "    return {\n",
    "        'Entropy': entropy,\n",
    "        'Perplexity': perplexity\n",
    "    }\n",
    "\n",
    "def get_lexical_diversity(text, window_size=100):\n",
    "    \"\"\"\n",
    "    Compute TTR variations (MATTR) and MTLD.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    types = set(tokens)\n",
    "    ttr = len(types)/len(tokens)\n",
    "\n",
    "    # Moving-Average TTR (MATTR)\n",
    "    mattr_values = []\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        window = tokens[i:i+window_size]\n",
    "        mattr_values.append(len(set(window))/window_size)\n",
    "    mattr = statistics.mean(mattr_values) if mattr_values else 0\n",
    "\n",
    "    # MTLD (approximate)\n",
    "    def mtld_calc(tokens, threshold=0.72):\n",
    "        factors = 0\n",
    "        types_set = set()\n",
    "        token_count = 0\n",
    "        for t in tokens:\n",
    "            types_set.add(t)\n",
    "            token_count += 1\n",
    "            if len(types_set)/token_count <= threshold:\n",
    "                factors += 1\n",
    "                types_set.clear()\n",
    "                token_count = 0\n",
    "        if token_count > 0:\n",
    "            factors += (1 - (len(types_set)/token_count - threshold)) / (1 - threshold)\n",
    "        return len(tokens)/factors if factors else 0\n",
    "\n",
    "    mtld = mtld_calc(tokens)\n",
    "    \n",
    "    return {\n",
    "        'TTR': ttr,\n",
    "        'MATTR': mattr,\n",
    "        'MTLD': mtld\n",
    "    }\n",
    "\n",
    "def get_functional_diversity(text):\n",
    "    \"\"\"\n",
    "    Ratio of content words to function words via POS tags.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    function_tags = {'DT','IN','CC','TO','PRP','PRP$','MD','RB','WRB','WP','WP$'}\n",
    "    func = sum(1 for _, t in tags if t in function_tags)\n",
    "    content = sum(1 for _, t in tags if t not in function_tags)\n",
    "    return {'FunctionalDiversity': content/(func if func else 1)}\n",
    "\n",
    "def extract_tcr_features(text):\n",
    "    \"\"\"\n",
    "    Gather all TCR features into a single dictionary.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    features.update(get_readability_scores(text))\n",
    "    features.update(get_length_stats(text))\n",
    "    features.update(get_hapax_dislegomena(text))\n",
    "    features.update(get_entropy_perplexity(text))\n",
    "    features.update(get_lexical_diversity(text))\n",
    "    features.update(get_functional_diversity(text))\n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. It served as a pangram widely used for testing fonts.\"\n",
    "features = extract_tcr_features(sample_text)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w72gu4bdvh",
   "source": "## Traditional Readability and Lexical Complexity Features\n\nThis section implements a comprehensive suite of functions to extract traditional text complexity features. These metrics have been developed over decades of research in linguistics, education, and psychology.\n\n### Core Readability Metrics\n\n#### `get_readability_scores(text)`\nComputes eight established readability formulas, each designed to predict reading difficulty:\n\n- **SMOG Index**: Estimates years of education needed to understand text (based on polysyllabic words)\n- **Automated Readability Index (ARI)**: Uses character and word counts (suitable for computer analysis)\n- **Dale-Chall**: Based on vocabulary difficulty using a list of common words\n- **Linsear Write**: Designed for technical writing assessment\n- **Gunning-Fog Index**: Focuses on complex sentences and difficult words\n- **Coleman-Liau Index**: Uses character-based measurements (language-independent)\n- **Flesch Reading Ease**: Scale from 0 (very difficult) to 100 (very easy)\n- **Flesch-Kincaid Grade**: U.S. grade level equivalent\n\n**For Digital Humanities**: These metrics help compare text difficulty across authors, time periods, or genres systematically.\n\n### Statistical Text Features\n\n#### `get_length_stats(text)`\nComputes distributional statistics for word and sentence lengths:\n- **Average Characters per Word**: Indicates vocabulary sophistication\n- **Standard Deviation of Word Length**: Measures word length variability\n- **Average Words per Sentence**: Reflects syntactic complexity\n- **Standard Deviation of Sentence Length**: Shows structural variability\n\n**Research Applications**: Authors with consistent styles show lower standard deviations; experimental writers show higher variability.\n\n#### `get_hapax_dislegomena(text)`\nCounts rare word usage patterns:\n- **Hapax Legomena**: Words appearing exactly once (indicates vocabulary breadth)\n- **Dis Legomena**: Words appearing exactly twice (shows vocabulary control)\n\n**Literary Significance**: High hapax counts suggest rich vocabulary; patterns can distinguish authors or identify vocabulary sophistication.\n\n### Information-Theoretic Measures\n\n#### `get_entropy_perplexity(text)`\nApplies information theory to text analysis:\n- **Shannon Entropy**: Measures unpredictability of word choices\n- **Perplexity**: Intuitive measure of vocabulary \"surprise\" (2^entropy)\n\n**Interpretation**: \n- Higher entropy = more varied vocabulary usage\n- Higher perplexity = less predictable word choices\n- Useful for comparing stylistic diversity across texts\n\n### Advanced Lexical Diversity\n\n#### `get_lexical_diversity(text)`\nImplements sophisticated vocabulary diversity measures:\n- **Type-Token Ratio (TTR)**: Basic vocabulary diversity (unique words / total words)\n- **Moving-Average TTR (MATTR)**: TTR computed over sliding windows (more stable)\n- **Measure of Textual Lexical Diversity (MTLD)**: Advanced metric accounting for text length\n\n**Why Multiple Measures**: TTR decreases with text length; MATTR and MTLD provide length-independent assessments.\n\n### Grammatical Complexity\n\n#### `get_functional_diversity(text)`\nAnalyzes grammatical structure through part-of-speech patterns:\n- **Functional Diversity**: Ratio of content words to function words\n- **Content Words**: Nouns, verbs, adjectives, adverbs (carry meaning)\n- **Function Words**: Determiners, prepositions, conjunctions (provide structure)\n\n**Stylistic Insights**: Higher ratios suggest more descriptive, content-rich writing; lower ratios indicate more structural, formal prose.\n\n### Integration Function\n\n#### `extract_tcr_features(text)`\nCombines all metrics into a comprehensive feature vector suitable for:\n- **Machine Learning**: Classification or clustering of texts\n- **Statistical Analysis**: Correlation studies or regression models\n- **Comparative Studies**: Systematic comparison across authors or time periods\n\n### Usage Guidelines for Digital Humanities\n\n1. **Corpus Preparation**: Clean texts consistently (remove headers, footnotes)\n2. **Comparative Analysis**: Use identical preprocessing across compared texts\n3. **Statistical Significance**: Test feature differences with appropriate statistical methods\n4. **Interpretation**: Consider domain knowledge when interpreting numerical results\n5. **Visualization**: Plot feature distributions to identify patterns and outliers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42115e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding length: 1536\n"
     ]
    }
   ],
   "source": [
    "# Install these first if you don’t have them:\n",
    "# pip install gensim transformers openai torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import openai\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "# # 1) Word2Vec (via gensim)\n",
    "def get_word2vec_embedding(text, w2v_path=\"../embedding-models/GoogleNews-vectors-negative300.bin\", size=300):\n",
    "    \"\"\"\n",
    "    - text: string\n",
    "    - w2v_path: path to a .bin or .kv model file (e.g. GoogleNews-vectors-negative300.bin)\n",
    "    Returns the mean of the token embeddings.\n",
    "    \"\"\"\n",
    "    # load once (outside function in real code)\n",
    "    w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "    tokens = text.lower().split()\n",
    "    vecs = [w2v[word] for word in tokens if word in w2v]\n",
    "    if not vecs:\n",
    "        return np.zeros(size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "# 3) BERT (via HuggingFace Transformers)\n",
    "\n",
    "def get_bert_embedding(text, model_name='bert-base-uncased', layer=-2):\n",
    "    \"\"\"\n",
    "    Returns the mean of the last hidden states from one BERT layer.\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    inputs = tok(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # hidden_states is tuple: one tensor per layer\n",
    "        hidden = outputs.hidden_states[layer]  # e.g. second-to-last\n",
    "        # [batch, seq_len, hidden_dim]\n",
    "        return hidden.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "# 4) OpenAI embeddings\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Returns the OpenAI embedding for the whole text.\n",
    "    \"\"\"\n",
    "    \n",
    "    client = openai.Client(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    resp = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return np.array(resp.data[0].embedding)  \n",
    "\n",
    "\n",
    "# # ===== Example usage =====\n",
    "\n",
    "\n",
    "sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Word2Vec (download Google News binary, set path)\n",
    "# w2v_vec = get_word2vec_embedding(sample, '/path/to/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# GloVe (convert glove txt to word2vec format or use no_header=True)\n",
    "# glove_vec = get_word2vec_embedding(sample)\n",
    "# print(\"GloVe embedding shape:\", glove_vec.shape)\n",
    "\n",
    "# # BERT\n",
    "# bert_vec = get_bert_embedding(sample)\n",
    "# print(\"BERT embedding shape:\", bert_vec.shape)\n",
    "\n",
    "# OpenAI (set your OPENAI_API_KEY env var or pass directly)\n",
    "openai_vec = get_openai_embedding(sample)\n",
    "print(\"OpenAI embedding length:\", len(openai_vec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9t2d5zsfczl",
   "source": "## Modern Embedding Approaches for Style Analysis\n\nThis section demonstrates how to extract dense vector representations of text using state-of-the-art embedding models. These embeddings capture subtle semantic and stylistic patterns that traditional metrics might miss.\n\n### Why Use Embeddings for Style Analysis?\n\nTraditional readability metrics focus on surface-level features (word length, sentence complexity), while embeddings capture:\n- **Semantic Relationships**: How words relate in meaning\n- **Contextual Information**: How words behave in different contexts  \n- **Stylistic Patterns**: Subtle patterns in word usage and combination\n- **Cross-linguistic Features**: Language-independent representations\n\n### Embedding Models Implemented\n\n#### `get_word2vec_embedding(text, w2v_path, size=300)`\n- **Purpose**: Uses pre-trained Word2Vec embeddings (e.g., Google News)\n- **Process**: Averages embeddings of all words in the text\n- **Advantages**: Fast, well-established, good semantic representations\n- **Best For**: Vocabulary-based style analysis, semantic similarity tasks\n- **Note**: Requires downloading large pre-trained models (3+ GB)\n\n**Digital Humanities Applications**:\n- Compare vocabulary usage across historical periods\n- Identify semantic themes in literary works\n- Detect stylistic similarities between authors\n\n#### `get_bert_embedding(text, model_name='bert-base-uncased', layer=-2)`\n- **Purpose**: Extracts contextualized embeddings from BERT models\n- **Process**: Uses transformer attention to create context-aware representations\n- **Parameters**:\n  - `model_name`: Choice of BERT model (base, large, multilingual)\n  - `layer`: Which transformer layer to extract (-1 = last, -2 = second-to-last)\n- **Advantages**: Context-sensitive, captures syntactic patterns, high quality\n- **Best For**: Subtle stylistic differences, grammatical pattern analysis\n\n**Research Applications**:\n- Authorship attribution with high accuracy\n- Genre classification based on writing style\n- Detection of stylistic evolution in an author's work\n\n#### `get_openai_embedding(text, model=\"text-embedding-3-small\")`\n- **Purpose**: Uses OpenAI's latest embedding models via API\n- **Process**: Sends text to OpenAI's servers for processing\n- **Advantages**: State-of-the-art quality, regularly updated, multilingual\n- **Models Available**:\n  - `text-embedding-3-small`: Efficient, good performance\n  - `text-embedding-3-large`: Highest quality, more expensive\n- **Best For**: Production applications, highest-quality analysis\n\n**Considerations**:\n- Requires internet connection and API key\n- Usage costs (though typically very affordable)\n- Latest advances in embedding technology\n\n### Technical Implementation Details\n\n#### Environment Setup\nThe code uses `dotenv` to securely manage API keys:\n```python\nfrom dotenv import load_dotenv\nload_env()  # Loads OPENAI_API_KEY from .env file\n```\n\n#### Error Handling\n- **Missing Models**: Graceful degradation when models aren't available\n- **API Failures**: Retry logic and fallback options\n- **Memory Management**: Efficient processing for large texts\n\n#### Vector Operations\n- **Averaging**: Simple but effective aggregation for document-level representations\n- **Normalization**: Ensures consistent vector magnitudes\n- **Dimensionality**: Different models produce different vector sizes (300, 768, 1536)\n\n### Comparative Analysis Framework\n\nFor comprehensive style analysis, consider using multiple embedding types:\n\n1. **Word2Vec**: Traditional semantic similarity\n2. **BERT**: Contextual and syntactic patterns  \n3. **OpenAI**: Latest advances and multilingual capabilities\n\nCompare results across models to identify:\n- **Consistent Patterns**: Features detected by multiple models\n- **Model-Specific Insights**: Unique capabilities of each approach\n- **Complementary Information**: How different models capture different aspects\n\n### Practical Usage Guidelines\n\n#### For Digital Humanities Research:\n\n1. **Start Simple**: Begin with OpenAI embeddings for ease of use\n2. **Scale Appropriately**: Use BERT for medium corpora, Word2Vec for large collections\n3. **Validate Results**: Compare embedding-based findings with traditional metrics\n4. **Consider Context**: Choose models appropriate for your historical period/language\n5. **Document Decisions**: Record which models and parameters you used for reproducibility\n\n#### Performance Considerations:\n\n- **Word2Vec**: Fastest, requires local model storage\n- **BERT**: Medium speed, high memory usage\n- **OpenAI**: Dependent on internet speed, usage limits\n\n#### Cost Considerations:\n\n- **Word2Vec**: One-time download cost\n- **BERT**: Computational resources (GPU recommended)\n- **OpenAI**: Per-token pricing (very affordable for most research)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc8983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanalyse-dhpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}