{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cab5500",
   "metadata": {},
   "source": [
    "# TCR Feature Extraction: Readability Formulas\n",
    "\n",
    "This module provides functions to compute standard readability metrics for a given text using the `textstat` library. These metrics estimate how easy or difficult a text is to read, often expressed as a U.S. grade level or a score.\n",
    "\n",
    "**Features computed:**\n",
    "- SMOG Index\n",
    "- Automated Readability Index (ARI)\n",
    "- Dale-Chall Readability Score\n",
    "- Linsear Write Formula\n",
    "- Gunning-Fog Index\n",
    "- Coleman-Liau Index\n",
    "- Flesch Reading Ease\n",
    "- Flesch Kincaid Grade Level\n",
    "\n",
    "**Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9190db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SMOG': 3.1291, 'ARI': 3.151578947368421, 'Dale-Chall': 9.094015789473683, 'Linsear Write': 3.75, 'Gunning-Fog': 3.8000000000000003, 'Coleman-Liau': 4.894736842105264, 'Flesch Reading Ease': 90.32934210526317, 'Flesch-Kincaid Grade': 3.0202631578947354, 'AvgChars/Word': 3.8095238095238093, 'StdChars/Word': 1.8157786633273922, 'AvgWords/Sentence': 10.5, 'StdWords/Sentence': 0.5, 'HapaxLegomena': 17, 'DisLegomena': 2, 'Entropy': 4.20184123230257, 'Perplexity': 18.402644982465112, 'TTR': 0.9047619047619048, 'MATTR': 0, 'MTLD': 7.212616822429907, 'FunctionalDiversity': 1.625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sali/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import textstat\n",
    "import math\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure necessary NLTK data is available\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_readability_scores(text):\n",
    "    \"\"\"\n",
    "    Compute core readability metrics.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'SMOG': textstat.smog_index(text),\n",
    "        'ARI': textstat.automated_readability_index(text),\n",
    "        'Dale-Chall': textstat.dale_chall_readability_score(text),\n",
    "        'Linsear Write': textstat.linsear_write_formula(text),\n",
    "        'Gunning-Fog': textstat.gunning_fog(text),\n",
    "        'Coleman-Liau': textstat.coleman_liau_index(text),\n",
    "        'Flesch Reading Ease': textstat.flesch_reading_ease(text),\n",
    "        'Flesch-Kincaid Grade': textstat.flesch_kincaid_grade(text)\n",
    "    }\n",
    "\n",
    "def get_length_stats(text):\n",
    "    \"\"\"\n",
    "    Average word length, sentence length, and their standard deviations.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chars_per_word = [len(w) for w in words]\n",
    "    words_per_sent = [len(nltk.word_tokenize(s)) for s in sentences]\n",
    "    \n",
    "    return {\n",
    "        'AvgChars/Word': statistics.mean(chars_per_word),\n",
    "        'StdChars/Word': statistics.pstdev(chars_per_word),\n",
    "        'AvgWords/Sentence': statistics.mean(words_per_sent),\n",
    "        'StdWords/Sentence': statistics.pstdev(words_per_sent)\n",
    "    }\n",
    "\n",
    "def get_hapax_dislegomena(text):\n",
    "    \"\"\"\n",
    "    Count words occurring exactly once (hapax) or twice (dis legomena).\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    freqs = Counter(tokens)\n",
    "    return {\n",
    "        'HapaxLegomena': sum(1 for c in freqs.values() if c == 1),\n",
    "        'DisLegomena': sum(1 for c in freqs.values() if c == 2)\n",
    "    }\n",
    "\n",
    "def get_entropy_perplexity(text):\n",
    "    \"\"\"\n",
    "    Shannon entropy and derived perplexity of the token distribution.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    freqs = Counter(tokens)\n",
    "    total = len(tokens)\n",
    "    entropy = -sum((c/total) * math.log2(c/total) for c in freqs.values())\n",
    "    perplexity = 2**entropy\n",
    "    return {\n",
    "        'Entropy': entropy,\n",
    "        'Perplexity': perplexity\n",
    "    }\n",
    "\n",
    "def get_lexical_diversity(text, window_size=100):\n",
    "    \"\"\"\n",
    "    Compute TTR variations (MATTR) and MTLD.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    types = set(tokens)\n",
    "    ttr = len(types)/len(tokens)\n",
    "\n",
    "    # Moving-Average TTR (MATTR)\n",
    "    mattr_values = []\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        window = tokens[i:i+window_size]\n",
    "        mattr_values.append(len(set(window))/window_size)\n",
    "    mattr = statistics.mean(mattr_values) if mattr_values else 0\n",
    "\n",
    "    # MTLD (approximate)\n",
    "    def mtld_calc(tokens, threshold=0.72):\n",
    "        factors = 0\n",
    "        types_set = set()\n",
    "        token_count = 0\n",
    "        for t in tokens:\n",
    "            types_set.add(t)\n",
    "            token_count += 1\n",
    "            if len(types_set)/token_count <= threshold:\n",
    "                factors += 1\n",
    "                types_set.clear()\n",
    "                token_count = 0\n",
    "        if token_count > 0:\n",
    "            factors += (1 - (len(types_set)/token_count - threshold)) / (1 - threshold)\n",
    "        return len(tokens)/factors if factors else 0\n",
    "\n",
    "    mtld = mtld_calc(tokens)\n",
    "    \n",
    "    return {\n",
    "        'TTR': ttr,\n",
    "        'MATTR': mattr,\n",
    "        'MTLD': mtld\n",
    "    }\n",
    "\n",
    "def get_functional_diversity(text):\n",
    "    \"\"\"\n",
    "    Ratio of content words to function words via POS tags.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    function_tags = {'DT','IN','CC','TO','PRP','PRP$','MD','RB','WRB','WP','WP$'}\n",
    "    func = sum(1 for _, t in tags if t in function_tags)\n",
    "    content = sum(1 for _, t in tags if t not in function_tags)\n",
    "    return {'FunctionalDiversity': content/(func if func else 1)}\n",
    "\n",
    "def extract_tcr_features(text):\n",
    "    \"\"\"\n",
    "    Gather all TCR features into a single dictionary.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    features.update(get_readability_scores(text))\n",
    "    features.update(get_length_stats(text))\n",
    "    features.update(get_hapax_dislegomena(text))\n",
    "    features.update(get_entropy_perplexity(text))\n",
    "    features.update(get_lexical_diversity(text))\n",
    "    features.update(get_functional_diversity(text))\n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The quick brown fox jumps over the lazy dog. It served as a pangram widely used for testing fonts.\"\n",
    "features = extract_tcr_features(sample_text)\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42115e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding length: 1536\n"
     ]
    }
   ],
   "source": [
    "# Install these first if you donâ€™t have them:\n",
    "# pip install gensim transformers openai torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import openai\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "\n",
    "# # 1) Word2Vec (via gensim)\n",
    "def get_word2vec_embedding(text, w2v_path=\"../embedding-models/GoogleNews-vectors-negative300.bin\", size=300):\n",
    "    \"\"\"\n",
    "    - text: string\n",
    "    - w2v_path: path to a .bin or .kv model file (e.g. GoogleNews-vectors-negative300.bin)\n",
    "    Returns the mean of the token embeddings.\n",
    "    \"\"\"\n",
    "    # load once (outside function in real code)\n",
    "    w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
    "    tokens = text.lower().split()\n",
    "    vecs = [w2v[word] for word in tokens if word in w2v]\n",
    "    if not vecs:\n",
    "        return np.zeros(size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "# 3) BERT (via HuggingFace Transformers)\n",
    "\n",
    "def get_bert_embedding(text, model_name='bert-base-uncased', layer=-2):\n",
    "    \"\"\"\n",
    "    Returns the mean of the last hidden states from one BERT layer.\n",
    "    \"\"\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    inputs = tok(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        # hidden_states is tuple: one tensor per layer\n",
    "        hidden = outputs.hidden_states[layer]  # e.g. second-to-last\n",
    "        # [batch, seq_len, hidden_dim]\n",
    "        return hidden.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "\n",
    "# 4) OpenAI embeddings\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Returns the OpenAI embedding for the whole text.\n",
    "    \"\"\"\n",
    "    \n",
    "    client = openai.Client(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "    resp = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return np.array(resp.data[0].embedding)  \n",
    "\n",
    "\n",
    "# # ===== Example usage =====\n",
    "\n",
    "\n",
    "sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Word2Vec (download Google News binary, set path)\n",
    "# w2v_vec = get_word2vec_embedding(sample, '/path/to/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# GloVe (convert glove txt to word2vec format or use no_header=True)\n",
    "# glove_vec = get_word2vec_embedding(sample)\n",
    "# print(\"GloVe embedding shape:\", glove_vec.shape)\n",
    "\n",
    "# # BERT\n",
    "# bert_vec = get_bert_embedding(sample)\n",
    "# print(\"BERT embedding shape:\", bert_vec.shape)\n",
    "\n",
    "# OpenAI (set your OPENAI_API_KEY env var or pass directly)\n",
    "openai_vec = get_openai_embedding(sample)\n",
    "print(\"OpenAI embedding length:\", len(openai_vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc8983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanalyse-dhpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
